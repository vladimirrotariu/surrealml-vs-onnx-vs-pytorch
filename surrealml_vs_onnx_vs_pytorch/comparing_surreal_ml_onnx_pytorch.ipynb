{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing `SurrealML` execution times with `ONNX` and `PyTorch` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [General dependencies and helpers](##general-dependencies-and-helpers)\n",
    "2. [Some words about SurrealML](##some-words-about-surrealml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General dependencies and helpers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by exporting some tools we will use for timing, and operating with SurrealDB/SurrrealML..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will use subprocess to upload our model via CLI programmatically\n",
    "import subprocess\n",
    "\n",
    "# time and wraps are used to define a chronometer decorator, in order to bring under a simple API the measurement of execution times\n",
    "import time\n",
    "from functools import wraps\n",
    "\n",
    "# We import the necessary classes from SurrealML, to be discussed immediately\n",
    "from surrealml import SurMlFile, Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some words about SurrealML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the [official docs](https://surrealdb.com/docs/surrealml):\n",
    "```\n",
    "SurrealML is an engine that seeks to do one thing, and one thing well: store and execute trained ML models. SurrealML does not intrude on the training frameworks that are already out there, instead works with them to ease the storage, loading, and execution of models. Someone using SurrealML will be able to train their model in a chosen framework in Python, save their model, and load and execute the model in either Python or Rust.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We aim to time the three cases that may be encountered in practice, namely:\n",
    "\n",
    "1. **SurrealML**: predicting with the model in ONNX format _inside_ the SurrealDB, and then fetching the prediction from SurrealDB.\n",
    "2. **PyTorch**: fetching the data from SurrealDB and _externally_ predicting with the PyTorch model.\n",
    "3. **ONNX**: fetching the data from SurrealDB and _externally_ predicting with the ONNX model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To handle timing in a consistent way, we define the decorator `chronometer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chronometer(foo):\n",
    "    @wraps(foo)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start = time.time()\n",
    "        _ = foo(*args, *kwargs)\n",
    "        end = time.time()\n",
    "        return end - start\n",
    "\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mFnn\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mModule):\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class Fnn(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(10, 5)\n",
    "        self.fc2 = nn.Linear(5, 1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return self.__class__.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Fnn()\n",
    "model.load_state_dict(torch.load(\"parameters.pth\")[\"model_state_dict\"])\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "example_input = torch.rand(1, 10)\n",
    "surml_file = SurMlFile(\n",
    "    model=model, name=str(model), inputs=example_input, engine=Engine.PYTORCH\n",
    ")\n",
    "\n",
    "path_surml = \"./model.surml\"\n",
    "surml_file.add_version(\"0.0.1\")\n",
    "surml_file.save(path_surml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The URL of the SurrealDB instance, and the authentication details, as well as the namespace and database in scope\n",
    "\"\"\"\n",
    "\n",
    "URL = \"http://localhost:8000\"\n",
    "NS = \"latency_test\"\n",
    "DB = \"surreal_ml_vs_pytorch\"\n",
    "USR = \"user\"\n",
    "PASS = \"user\"\n",
    "CRD = (USR, PASS)\n",
    "PATH = \"file://surreal\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The CLI command to import the model in the SurrealDB database of choice, this time we use it programmatically\n",
    "\"\"\"\n",
    "\n",
    "command = [\n",
    "    \"surreal\",\n",
    "    \"ml\",\n",
    "    \"import\",\n",
    "    \"--endpoint\",\n",
    "    URL,\n",
    "    \"--user\",\n",
    "    USR,\n",
    "    \"--pass\",\n",
    "    PASS,\n",
    "    \"--ns\",\n",
    "    NS,\n",
    "    \"--db\",\n",
    "    DB,\n",
    "    path_surml,\n",
    "]\n",
    "\n",
    "\n",
    "subprocess.Popen(\n",
    "    command,\n",
    "    stdin=subprocess.PIPE,\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.PIPE,\n",
    ")\n",
    "output = subprocess.check_output(command, stderr=subprocess.STDOUT)\n",
    "output_str = output.decode(\"utf-8\")\n",
    "print(output_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\"\"\"\n",
    "###################################################################################################\n",
    "--> The other way to upload a model to SurrealDB. Could not make it work by the time of writing :( \n",
    "###################################################################################################\n",
    "\n",
    "surml_file.upload(\n",
    "    path=\"./model.surml\",\n",
    "    url=URL,\n",
    "    chunk_size=36864,\n",
    "    namespace=NS,\n",
    "    database=DB,\n",
    "    username=USR,\n",
    "    password=PASS)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The code that was used to generate the test data, and load it in SurrealDB\n",
    "\"\"\"\n",
    "\n",
    "from surrealist import Surreal\n",
    "\n",
    "surreal = Surreal(\n",
    "    url=URL,\n",
    "    namespace=NS,\n",
    "    database=DB,\n",
    "    credentials=CRD,\n",
    "    log_level=\"ERROR\",\n",
    "    timeout=10**4,\n",
    ")\n",
    "\n",
    "max_test_size = 10**4\n",
    "# chunk_size should divide max_test_size, we do it as we create the test inputs in memory, and thus we should avoid a memory crash.\n",
    "chunk_size = 10**2\n",
    "number_chunks = int(max_test_size / chunk_size)\n",
    "\n",
    "# likewise, test_step should divide max_test_size\n",
    "test_step = 10**3\n",
    "number_steps = int(max_test_size / test_step)\n",
    "\n",
    "with surreal.connect() as connect:\n",
    "    for _ in range(number_chunks):\n",
    "        test_inputs = torch.rand(chunk_size, 10).tolist()\n",
    "        [\n",
    "            connect.query(f\"CREATE inputs:ulid() SET value = {input};\")\n",
    "            for input in test_inputs\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surreal_times = []\n",
    "try:\n",
    "    with surreal.connect() as connect:\n",
    "        for increment in range(number_steps):\n",
    "            test_size = (increment + 1) * test_step\n",
    "\n",
    "            # in a prior run, the query result was tested with assert query_result[\"status\"] == \"OK\"\n",
    "            # also do print(query_result) to check further\n",
    "            @chronometer\n",
    "            def evaluate_with_surrealdb():\n",
    "                _ = connect.query(\n",
    "                    f\"SELECT VALUE ml::Fnn<0.0.1>(value) FROM inputs LIMIT {test_size};\"\n",
    "                ).to_dict()[\"result\"]\n",
    "\n",
    "            elapsed_time = evaluate_with_surrealdb()\n",
    "            print(f\"For {test_size} datapoints, it took {elapsed_time} seconds\")\n",
    "            surreal_times.append(elapsed_time)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_times = []\n",
    "try:\n",
    "    with surreal.connect() as connect:\n",
    "        for increment in range(1, 11):\n",
    "            test_size = increment * test_step\n",
    "\n",
    "            @chronometer\n",
    "            def evaluate_pytorch():\n",
    "                inputs = connect.query(\n",
    "                    f\"SELECT VALUE value FROM inputs LIMIT {test_size}\"\n",
    "                ).to_dict()[\"result\"]\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    _ = model.forward(torch.tensor(inputs))\n",
    "\n",
    "            elapsed_time = evaluate_pytorch()\n",
    "            print(f\"For {test_size} datapoints, it took {elapsed_time} seconds\")\n",
    "            pytorch_times.append(elapsed_time)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
