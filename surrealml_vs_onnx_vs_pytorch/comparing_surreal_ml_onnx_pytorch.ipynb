{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing execution times: `SurrealML` vs `ONNX` vs `PyTorch` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [General dependencies and helpers](#general-dependencies-and-helpers)\n",
    "2. [Some words about SurrealML](#some-words-about-surrealml)\n",
    "3. [Problem refinement](#problem-refinement)\n",
    "4. [A typical neural network](#a-typical-neural-network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General dependencies and helpers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by exporting some tools we will use for timing, and operating with SurrealDB/SurrrealML..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will use subprocess to upload our model via CLI programmatically\n",
    "import subprocess\n",
    "\n",
    "# We import the necessary classes from SurrealML, to be discussed immediately\n",
    "from surrealml import SurMlFile, Engine\n",
    "\n",
    "# We import the main PyTorch module, as well as the module for operating with neural networks\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# time and wraps are used to define the chronometer decorator\n",
    "import time\n",
    "from functools import wraps\n",
    "\n",
    "# the chronometer decorator brings an unified API for measuring execution times\n",
    "def chronometer(foo):\n",
    "    @wraps(foo)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start = time.time()\n",
    "        _ = foo(*args, *kwargs)\n",
    "        end = time.time()\n",
    "        return end - start\n",
    "\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some words about SurrealML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the [official docs](https://surrealdb.com/docs/surrealml):\n",
    "    \n",
    "```\n",
    "SurrealML is an engine that seeks to do one thing, and one thing well: store and execute trained ML models. SurrealML does not intrude on the training frameworks that are already out there, instead works with them to ease the storage, loading, and execution of models. Someone using SurrealML will be able to train their model in a chosen framework in Python, save their model, and load and execute the model in either Python or Rust.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically, we aim to develop and train models using `PyTorch/scikit-learn/Tensorflow/linfa`, and then load them to SurrealDB.\n",
    "\n",
    "Inside SurrealDB, a model is represented in the [.surml format](https://surrealdb.com/docs/surrealml/storage#the-anatomy-of-a-surml-file). Schematically, from top to bottom of a .surml file, we roughly have that:\n",
    "```\n",
    ".surml file =  a 4 byte integer + variable metadata whose length is specified by the 4 bytes integer + model parameters in the ONNX format\n",
    "```\n",
    "A .surml file is loaded by starting with the 4 bytes integer, and then using it to determine the length of the model metadata; once the model metadata has been loaded, the loader assumes that the rest is ONNX protobuf, and parses it accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the time of writing, in the [source code](https://github.com/surrealdb/surrealml/blob/main/surrealml/engine/__init__.py) of the `Engine` enum, we have the following docstring:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attributes:\n",
    "- **PYTORCH**: The PyTorch engine which will be PyTorch and ONNX.\n",
    "- **NATIVE**: The native engine which will be native Rust and Linfa.\n",
    "- **SKLEARN**: The scikit-learn engine which will be scikit-learn and ONNX.\n",
    "- **TENSORFLOW**: The TensorFlow engine which will be TensorFlow and ONNX.\n",
    "- **ONNX**: The ONNX engine which bypasses the conversion to ONNX."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we may infer that, for the sake of comparing `SurrealML` vs `ONNX` vs `PyTorch`, for the same model, it should be equivalent using `Engine.PYTORCH`/`Engine.SKLEARN`/`Engine.TENSORFLOW`, as irrespective of the framework used, the model will be exported to the ONNX first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem refinement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We single out three cases that may be encountered in practice, namely:\n",
    "\n",
    "1. **Execute with SurrealML[inside SurrealDB] && fetch data from SurrealDB [optional]**: predicting with the model in .surml format _inside_ the SurrealDB, and then _optionally_ fetching the prediction from SurrealDB.\n",
    "2. **Fetch data from SurrealDB && execute with PyTorch**: fetching the data from SurrealDB and _externally_ predicting with the PyTorch model.\n",
    "3. **Fetch data from SurrealDB && execute with ONNX runtime**: fetching the data from SurrealDB and _externally_ predicting with the ONNX model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the 3 scenarios above, one may deduct the following benefits of using SurrealML:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Reduced Database Transactions**\n",
    "  - No need to fetch data from SurrealDB if predictions are not consumed immediately.\n",
    "  - Eliminates at least **2 database transactions**.\n",
    "\n",
    "- **Improved Security**\n",
    "  - Operates on the input used for predictions, as well as on the calculated predictions, without needing to retrieve it from the database, enhancing security.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, one may be curious about the `performance` of SurrealML, so we will provide an implementation of an experiment to measure just this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A typical neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mFnn\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mModule):\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(10, 5)\n",
    "        self.fc2 = nn.Linear(5, 1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return self.__class__.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Fnn()\n",
    "model.load_state_dict(torch.load(\"parameters.pth\")[\"model_state_dict\"])\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "example_input = torch.rand(1, 10)\n",
    "surml_file = SurMlFile(\n",
    "    model=model, name=str(model), inputs=example_input, engine=Engine.PYTORCH\n",
    ")\n",
    "\n",
    "path_surml = \"./model.surml\"\n",
    "surml_file.add_version(\"0.0.1\")\n",
    "surml_file.save(path_surml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The URL of the SurrealDB instance, and the authentication details, as well as the namespace and database in scope\n",
    "\"\"\"\n",
    "\n",
    "URL = \"http://localhost:8000\"\n",
    "NS = \"latency_test\"\n",
    "DB = \"surreal_ml_vs_pytorch\"\n",
    "USR = \"user\"\n",
    "PASS = \"user\"\n",
    "CRD = (USR, PASS)\n",
    "PATH = \"file://surreal\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The CLI command to import the model in the SurrealDB database of choice, this time we use it programmatically\n",
    "\"\"\"\n",
    "\n",
    "command = [\n",
    "    \"surreal\",\n",
    "    \"ml\",\n",
    "    \"import\",\n",
    "    \"--endpoint\",\n",
    "    URL,\n",
    "    \"--user\",\n",
    "    USR,\n",
    "    \"--pass\",\n",
    "    PASS,\n",
    "    \"--ns\",\n",
    "    NS,\n",
    "    \"--db\",\n",
    "    DB,\n",
    "    path_surml,\n",
    "]\n",
    "\n",
    "\n",
    "subprocess.Popen(\n",
    "    command,\n",
    "    stdin=subprocess.PIPE,\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.PIPE,\n",
    ")\n",
    "output = subprocess.check_output(command, stderr=subprocess.STDOUT)\n",
    "output_str = output.decode(\"utf-8\")\n",
    "print(output_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\"\"\"\n",
    "###################################################################################################\n",
    "--> The other way to upload a model to SurrealDB. Could not make it work by the time of writing :( \n",
    "###################################################################################################\n",
    "\n",
    "surml_file.upload(\n",
    "    path=\"./model.surml\",\n",
    "    url=URL,\n",
    "    chunk_size=36864,\n",
    "    namespace=NS,\n",
    "    database=DB,\n",
    "    username=USR,\n",
    "    password=PASS)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The code that was used to generate the test data, and load it in SurrealDB\n",
    "\"\"\"\n",
    "\n",
    "from surrealist import Surreal\n",
    "\n",
    "surreal = Surreal(\n",
    "    url=URL,\n",
    "    namespace=NS,\n",
    "    database=DB,\n",
    "    credentials=CRD,\n",
    "    log_level=\"ERROR\",\n",
    "    timeout=10**4,\n",
    ")\n",
    "\n",
    "max_test_size = 10**4\n",
    "# chunk_size should divide max_test_size, we do it as we create the test inputs in memory, and thus we should avoid a memory crash.\n",
    "chunk_size = 10**2\n",
    "number_chunks = int(max_test_size / chunk_size)\n",
    "\n",
    "# likewise, test_step should divide max_test_size\n",
    "test_step = 10**3\n",
    "number_steps = int(max_test_size / test_step)\n",
    "\n",
    "with surreal.connect() as connect:\n",
    "    for _ in range(number_chunks):\n",
    "        test_inputs = torch.rand(chunk_size, 10).tolist()\n",
    "        [\n",
    "            connect.query(f\"CREATE inputs:ulid() SET value = {input};\")\n",
    "            for input in test_inputs\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surreal_times = []\n",
    "try:\n",
    "    with surreal.connect() as connect:\n",
    "        for increment in range(number_steps):\n",
    "            test_size = (increment + 1) * test_step\n",
    "\n",
    "            # in a prior run, the query result was tested with assert query_result[\"status\"] == \"OK\"\n",
    "            # also do print(query_result) to check further\n",
    "            @chronometer\n",
    "            def evaluate_with_surrealdb():\n",
    "                _ = connect.query(\n",
    "                    f\"SELECT VALUE ml::Fnn<0.0.1>(value) FROM inputs LIMIT {test_size};\"\n",
    "                ).to_dict()[\"result\"]\n",
    "\n",
    "            elapsed_time = evaluate_with_surrealdb()\n",
    "            print(f\"For {test_size} datapoints, it took {elapsed_time} seconds\")\n",
    "            surreal_times.append(elapsed_time)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_times = []\n",
    "try:\n",
    "    with surreal.connect() as connect:\n",
    "        for increment in range(1, 11):\n",
    "            test_size = increment * test_step\n",
    "\n",
    "            @chronometer\n",
    "            def evaluate_pytorch():\n",
    "                inputs = connect.query(\n",
    "                    f\"SELECT VALUE value FROM inputs LIMIT {test_size}\"\n",
    "                ).to_dict()[\"result\"]\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    _ = model.forward(torch.tensor(inputs))\n",
    "\n",
    "            elapsed_time = evaluate_pytorch()\n",
    "            print(f\"For {test_size} datapoints, it took {elapsed_time} seconds\")\n",
    "            pytorch_times.append(elapsed_time)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
