{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing execution times: `SurrealML` vs `ONNX` vs `PyTorch` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "\n",
    "1.  <a href=\"#1\">General dependencies and helpers</a>\n",
    "2.  <a href=\"#2\">Some words about SurrealML</a>\n",
    "3.  <a href=\"#3\">Problem refinement</a>\n",
    "4.  <a href=\"#4\">A typical neural network</a>\n",
    "5.  <a href=\"#5\">Loading the model to SurrealDB</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"1\"></h2>\n",
    "\n",
    "## General dependencies and helpers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by exporting some tools we will use for timing, and operating with SurrealDB/SurrrealML..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will use subprocess to upload our model via CLI programmatically\n",
    "import subprocess\n",
    "\n",
    "# We import the necessary classes from SurrealML, to be discussed immediately\n",
    "from surrealml import SurMlFile, Engine\n",
    "\n",
    "# We import the main PyTorch module, as well as the module for operating with neural networks\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# time and wraps are used to define the chronometer decorator\n",
    "import time\n",
    "from functools import wraps\n",
    "\n",
    "# the chronometer decorator brings an unified API for measuring execution times\n",
    "def chronometer(foo):\n",
    "    @wraps(foo)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start = time.time()\n",
    "        _ = foo(*args, *kwargs)\n",
    "        end = time.time()\n",
    "        return end - start\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "from IPython.core.magic import register_cell_magic\n",
    "\n",
    "# This magic command allows us to skip the execution of a specific cell\n",
    "@register_cell_magic\n",
    "def skip(line, cell):\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"2\"></h2>\n",
    "\n",
    "## Some words about SurrealML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the [official docs](https://surrealdb.com/docs/surrealml):\n",
    "    \n",
    "```\n",
    "SurrealML is an engine that seeks to do one thing, and one thing well: store and execute trained ML models. SurrealML does not intrude on the training frameworks that are already out there, instead works with them to ease the storage, loading, and execution of models. Someone using SurrealML will be able to train their model in a chosen framework in Python, save their model, and load and execute the model in either Python or Rust.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically, we aim to develop and train models using `PyTorch/scikit-learn/Tensorflow/linfa`, and then load them to SurrealDB.\n",
    "\n",
    "Inside SurrealDB, a model is represented in the [.surml format](https://surrealdb.com/docs/surrealml/storage#the-anatomy-of-a-surml-file). Schematically, from top to bottom of a .surml file, we roughly have that:\n",
    "\n",
    "`.surml file` =  `4 byte integer` + `variable metadata [size specified by 4 bytes integer]` + `model parameters [ONNX format]`\n",
    "\n",
    "A .surml file is loaded by starting with the 4 bytes integer, and then using it to determine the length of the model metadata; once the model metadata has been loaded, the loader assumes that the rest is ONNX protobuf, and parses it accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the time of writing, in the [source code](https://github.com/surrealdb/surrealml/blob/main/surrealml/engine/__init__.py) of the `Engine` enum, we have the following docstring:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Attributes_:\n",
    "- **PYTORCH**: The PyTorch engine which will be PyTorch and ONNX.\n",
    "- **NATIVE**: The native engine which will be native Rust and Linfa.\n",
    "- **SKLEARN**: The scikit-learn engine which will be scikit-learn and ONNX.\n",
    "- **TENSORFLOW**: The TensorFlow engine which will be TensorFlow and ONNX.\n",
    "- **ONNX**: The ONNX engine which bypasses the conversion to ONNX."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we may infer that, for the sake of comparing `SurrealML` vs `ONNX` vs `PyTorch`, for the same model, it should be equivalent using `Engine.PYTORCH`/`Engine.SKLEARN`/`Engine.TENSORFLOW`, as irrespective of the framework used, the model will be exported to the ONNX first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a>\n",
    "\n",
    "## Problem refinement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We single out three cases that may be encountered in practice, namely:\n",
    "\n",
    "1. **Execute with SurrealML[inside SurrealDB] && fetch data from SurrealDB [optional]**: predicting with the model in .surml format _inside_ the SurrealDB, and then _optionally_ fetching the prediction from SurrealDB.\n",
    "2. **Fetch data from SurrealDB && execute with PyTorch**: fetching the data from SurrealDB and _externally_ predicting with the PyTorch model.\n",
    "3. **Fetch data from SurrealDB && execute with ONNX runtime**: fetching the data from SurrealDB and _externally_ predicting with the ONNX model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the 3 scenarios above, one may deduct the following benefits of using SurrealML:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Reduced Database Transactions**\n",
    "  - No need to fetch data from SurrealDB if predictions are not consumed immediately.\n",
    "  - Eliminates at least **2 database transactions**, if fetching the input data and inserting the computed predictions is not needed anymore.\n",
    "\n",
    "- **Improved Security**\n",
    "  - Operates on the input used for predictions, as well as on the calculated predictions, without needing to retrieve it from the database, enhancing security.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, one may be curious about the `performance` of SurrealML, so we will provide an implementation of an experiment to measure just this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"4\"></h2>\n",
    "\n",
    "## A toy neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, we define `ToyNet`, which is a two-layer feedforward neural network with ReLU activation. It consists of an input layer with 10 features, a hidden layer of 5 neurons (`fc1`), and an output layer of 1 neuron (`fc2`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToyNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(10, 5)\n",
    "        self.fc2 = nn.Linear(5, 1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return self.__class__.__name__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and then we instantiate the model, and load a persistent version of the randomly intialized parameters of the model, from a previous run: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleNet(\n",
       "  (fc1): Linear(in_features=10, out_features=5, bias=True)\n",
       "  (fc2): Linear(in_features=5, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ToyNet()\n",
    "torch.save(model.state_dict(),\"./params.pth\")\n",
    "model.load_state_dict(torch.load(\"params.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"5\"></h2>\n",
    "\n",
    "## Loading the model to SurrealDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we know from the Engine docstring, under the hood SurrealML converts any PyTorch/scikit-learn/Tensorflow model to the ONNX format, hence we switch the model to inference mode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `SurMlFile` object comes in handy to save our model in the `.surml` format. As our model was developed using PyTorch, we select the `Engine.PYTORCH` option:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Owing to the fact that the SURML format builds on the ONNX format, we have to specify an example input\n",
    "example_input = torch.rand(1, 10)\n",
    "surml_file = SurMlFile(\n",
    "    model=model, name=str(model), inputs=example_input, engine=Engine.PYTORCH\n",
    ")\n",
    "\n",
    "# we also choose a local path where to save the model, as well as a version of the model\n",
    "path_surml = \"./model.surml\"\n",
    "surml_file.add_version(\"0.0.1\")\n",
    "surml_file.save(path_surml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is high time to load our model, now represented by the file `model.surml`, to SurrealDB.\n",
    "In the directory where this notebook is located, there is a script that downloads and sets up `SurrealDB`, `SurrealML`, and the `Rust compiler`. In order to use it, with the active directory the directory of this notebook, execute the following commands in a bash shell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "chmod +x ./set_surreal_ml.sh\n",
    "./set_surreal_ml.sh 4444"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I decided to start a SurrealDB instance `in-memory`, ready to be accessed at port `4444` of `localhost`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The URL of the SurrealDB instance, as well as the namespace and database in scope\n",
    "\"\"\"\n",
    "\n",
    "URL = \"http://localhost:4444\"\n",
    "NS = \"comparison_test\"\n",
    "DB = \"surrealml_vs_onnx_vs_pytorch\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The CLI command to import the model in the SurrealDB database of choice, this time we use it programmatically\n",
    "\"\"\"\n",
    "\n",
    "command = [\n",
    "    \"surreal\",\n",
    "    \"ml\",\n",
    "    \"import\",\n",
    "    \"--endpoint\",\n",
    "    URL,\n",
    "    \"--user\",\n",
    "    USR,\n",
    "    \"--pass\",\n",
    "    PASS,\n",
    "    \"--ns\",\n",
    "    NS,\n",
    "    \"--db\",\n",
    "    DB,\n",
    "    path_surml,\n",
    "]\n",
    "\n",
    "\n",
    "subprocess.Popen(\n",
    "    command,\n",
    "    stdin=subprocess.PIPE,\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.PIPE,\n",
    ")\n",
    "output = subprocess.check_output(command, stderr=subprocess.STDOUT)\n",
    "output_str = output.decode(\"utf-8\")\n",
    "print(output_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\"\"\"\n",
    "###################################################################################################\n",
    "--> The other way to upload a model to SurrealDB. Could not make it work by the time of writing :( \n",
    "###################################################################################################\n",
    "\n",
    "surml_file.upload(\n",
    "    path=\"./model.surml\",\n",
    "    url=URL,\n",
    "    chunk_size=36864,\n",
    "    namespace=NS,\n",
    "    database=DB,\n",
    "    username=USR,\n",
    "    password=PASS)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The code that was used to generate the test data, and load it in SurrealDB\n",
    "\"\"\"\n",
    "\n",
    "from surrealist import Surreal\n",
    "\n",
    "surreal = Surreal(\n",
    "    url=URL,\n",
    "    namespace=NS,\n",
    "    database=DB,\n",
    "    credentials=CRD,\n",
    "    log_level=\"ERROR\",\n",
    "    timeout=10**4,\n",
    ")\n",
    "\n",
    "max_test_size = 10**4\n",
    "# chunk_size should divide max_test_size, we do it as we create the test inputs in memory, and thus we should avoid a memory crash.\n",
    "chunk_size = 10**2\n",
    "number_chunks = int(max_test_size / chunk_size)\n",
    "\n",
    "# likewise, test_step should divide max_test_size\n",
    "test_step = 10**3\n",
    "number_steps = int(max_test_size / test_step)\n",
    "\n",
    "with surreal.connect() as connect:\n",
    "    for _ in range(number_chunks):\n",
    "        test_inputs = torch.rand(chunk_size, 10).tolist()\n",
    "        [\n",
    "            connect.query(f\"CREATE inputs:ulid() SET value = {input};\")\n",
    "            for input in test_inputs\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surreal_times = []\n",
    "try:\n",
    "    with surreal.connect() as connect:\n",
    "        for increment in range(number_steps):\n",
    "            test_size = (increment + 1) * test_step\n",
    "\n",
    "            # in a prior run, the query result was tested with assert query_result[\"status\"] == \"OK\"\n",
    "            # also do print(query_result) to check further\n",
    "            @chronometer\n",
    "            def evaluate_with_surrealdb():\n",
    "                _ = connect.query(\n",
    "                    f\"SELECT VALUE ml::Fnn<0.0.1>(value) FROM inputs LIMIT {test_size};\"\n",
    "                ).to_dict()[\"result\"]\n",
    "\n",
    "            elapsed_time = evaluate_with_surrealdb()\n",
    "            print(f\"For {test_size} datapoints, it took {elapsed_time} seconds\")\n",
    "            surreal_times.append(elapsed_time)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_times = []\n",
    "try:\n",
    "    with surreal.connect() as connect:\n",
    "        for increment in range(1, 11):\n",
    "            test_size = increment * test_step\n",
    "\n",
    "            @chronometer\n",
    "            def evaluate_pytorch():\n",
    "                inputs = connect.query(\n",
    "                    f\"SELECT VALUE value FROM inputs LIMIT {test_size}\"\n",
    "                ).to_dict()[\"result\"]\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    _ = model.forward(torch.tensor(inputs))\n",
    "\n",
    "            elapsed_time = evaluate_pytorch()\n",
    "            print(f\"For {test_size} datapoints, it took {elapsed_time} seconds\")\n",
    "            pytorch_times.append(elapsed_time)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
